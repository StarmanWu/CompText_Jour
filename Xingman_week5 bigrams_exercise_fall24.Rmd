---
title: "Bigrams Exercise Sept 24"
author: "Rob Wells"
date: '2024-09-20'
output: html_document
---

# Jour 389/689 Fall 2024:


```{r}
#load tidyverse, tidytext, rio and quanteda libraries
library(tidyverse)
library(tidytext)
library(rio)
library(quanteda)
```

```{r}
#Import dataframe 
lynch <- read_csv("articles_oct_19.csv")
head(lynch)
```


# Create a new dataframe that filters articles for 1900 to 1910

```{r}
newdf <- lynch %>% filter(year >= 1900 & year <= 1910)
newdf

```


# Count the number of distinct articles in 1900 dataframe
```{r}
newdf %>% 
  select(file_id) %>% # I assumed that variable file_id refers to the ID of articles. 
  # Therefore, counting the number of distinct articles means counting the number of different IDs.
 distinct(file_id, .keep_all = TRUE) %>% 
  count(file_id) %>% 
  summarize(total =sum(n)) 
# There are 1322 distinct articles in total.
```

# Count the number of newspaper_states in the 1900 corpus
```{r}

newdf %>% 
  select(newspaper_state) %>% # I assumed that this prompt is asking about the number of distinct states.
  distinct(newspaper_state, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  summarize(total =sum(n)) 
# There are 48 states in total.

newdf %>% 
  select(newspaper_state, file_id) %>% 
  distinct(file_id, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))
# Here is a table including the number of articles in each state. 


```

# Tokenize the 1900 lynching stories

```{r}
stories <- str_replace_all(newdf$sentence, "- ", "")

stories_df <- tibble(stories,)
stories_df

# unnest includes lower, punct removal

stories_tokenized <- stories_df %>%
  unnest_tokens(word,stories)

stories_tokenized # turned into words

```


#Remove stopwords
The tidytext package includes the stop_words dataset.It contains, as of this writing, 1,149 words that data scientists and linguistic nerds felt could be removed from sentences because they don't add meaning. Filtering out these words can help focus on the more meaningful content, making it easier to uncover trends, themes, and key information in large amounts of text. Obviously, we have different priorities and we may or may not want to use stop_words or we have want to provide a customized list of stop words.

The stop_words list is derived from three separate lists, or lexicons: SMART (571 words), onix (404 words), and snowball (174 words)

The ONIX lexicon comes from the Open Information Exchange and is often used in text mining and natural language processing. 

The Snowball lexicon is part of a broader project that has algorithms that simplify words in different languages by reducing them to their root form. It's best known for the Porter stemming algorithm, which, for example, changes "running" to "run." 

Lastly, the SMART lexicon is a set of common words, like "and," "the," and "is," and it comes from the SMART Information Retrieval System, created at Cornell University in the 1960s.

```{r}
data(stop_words)

test <- stop_words %>% 
  as.data.frame()

head(test)
```
# Strip out stop words

```{r}

stories_tokenized <- stories_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))
stories_tokenized 
# Word Count

story_word_ct <- stories_tokenized %>%
  count(word, sort=TRUE)

head(story_word_ct)


```

# Bigrams
## We are now creating two word phrases but before the stop words are taken out

```{r}
stories_bigrams <- stories_df %>%
  unnest_tokens(bigram, stories, token="ngrams", n=2)
stories_bigrams
stories_bigrams_separated <- stories_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")
stories_bigrams_separated 
```

# Create a new dataframe with counts of the bigrams
```{r}
dfbi<- stories_bigrams_separated  %>% 
  count(word1, word2, sort = TRUE)
dfbi
```

## Now filter the counts 
```{r}

stories_bigrams_filtered <- stories_bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

stories_bigram_cts2 <- stories_bigrams_filtered %>%
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

stories_bigram_cts2
```

# Add a "1900" decade column

Hint: use mutate

```{r}
stories_bigram_cts2<- stories_bigram_cts2 %>% 
  mutate(decade = "1900")
stories_bigram_cts2

```


# YOUR TURN

Create one dataframe with black press articles
```{r}
bdf<- lynch %>% 
  filter(black_press == "Y")
head(bdf)
bdf
#bdf is the new dataframe including black press articles
```


Create a second dataframe without black press articles
```{r}
nbdf <- lynch %>%
  filter(is.na(black_press) | black_press != "Y")
head(nbdf)


```



Produce the top 20 bigrams for the black press and non-black press coverage
```{r}
# In terms of black press
stories <- str_replace_all(bdf$sentence, "- ", "")
stories_bdf <- tibble(stories,)
stories_bdf # Tokenize

bdf_bigrams <- stories_bdf %>%
  unnest_tokens(bigram, stories, token="ngrams", n=2)

bdf_bigrams_separated <- bdf_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  count(word1, word2, sort = TRUE)
bdf_bigrams_separated 

bdf_bigrams_filtered <- bdf_bigrams_separated  %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
bdf_bigrams_filtered

bdf_bigrams_filtered  <- bdf_bigrams_filtered  %>%
  filter(!is.na(word1))
bdf_bigrams_filtered 

bdf_bigrams_filtered <- bdf_bigrams_filtered %>% 
  slice_max(n, n=20) 
bdf_bigrams_filtered 

# In terms of non-black press
stories <- str_replace_all(nbdf$sentence, "- ", "")
stories_nbdf <- tibble(stories,)
stories_nbdf # Tokenize

nbdf_bigrams <- stories_nbdf %>%
  unnest_tokens(bigram, stories, token="ngrams", n=2)

nbdf_bigrams_separated <- nbdf_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  count(word1, word2, sort = TRUE)
nbdf_bigrams_separated 

nbdf_bigrams_filtered <- nbdf_bigrams_separated  %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
nbdf_bigrams_filtered 

nbdf_bigrams_filtered  <- nbdf_bigrams_filtered  %>%
  filter(!is.na(word1))
nbdf_bigrams_filtered 

nbdf_bigrams_filtered <- nbdf_bigrams_filtered %>% 
  slice_max(n, n=20) 
nbdf_bigrams_filtered 
```


Compare and discuss!

```{r}
# Combine data frames side by side
combined_df <- cbind(bdf_bigrams_filtered, nbdf_bigrams_filtered)
combined_df
# Discussion: Two data frames and a shared focus but also different emphases.
# 1. Shared focus: in both black-press and non-black-press bigram data frames, the bigram "lynch law" ranks high, which indicates a shared focus on topics related to lynching;
# 2. Civil rights and equality in black-press data frame: bigrams "colored people" and "national association" rank highly compared to non-black-press data frame;
# 3. Violence, crime, and races in non-black-press data frame: bigrams "negro lynched", "county jail", "court house", and "mob -" rank highly compared to black-press data frame.
```

